{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors\n",
    "---\n",
    "K-Nearest Neighbors is a supervised machine learning algorithm that looks at the closest *k*  existing points to a specific data point, then looks at the labels of those points in order to estimate the label of the new point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be assigning *k* a value of 4, which means we will be looking at the 4 closest points to predict the label of any new data.\n",
    "\n",
    "In order to determine what the closest points are, we will be using the **Distance Formula** to calculate the distance between two points ($p_1$ and $p_2$) which is: $$d(p_1,p_2)=\\sqrt{(x_{p_1}-x_{p_2})^2+(y_{p_1}-y_{p_2})^2}$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "distance (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distance formula function\n",
    "function distance(p1, p2)\n",
    "    return sqrt(sum((p1[i]-p2[i])^2 for i in 1:length(p1)))\n",
    "    end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Once we have the distance function established, we then need to apply it to our data to get the distance from the test point to every other point in our data. After that we can sort the results and return the first *k* results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNN (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNN Algorithm\n",
    "function KNN(target, x_array, label_array, k)\n",
    "    \n",
    "    # Determine distance between target point and all other points\n",
    "    distance_array = [(x_array[i], label_array[i], distance(target, x_array[i]))\n",
    "                        for i = 1:length(x_array) \n",
    "                        if target != x_array[i]\n",
    "                        ]\n",
    "    \n",
    "    # Find and return 'k' closest points\n",
    "    sort!(distance_array, by = x -> x[3])\n",
    "    return distance_array[1:k]\n",
    "end "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "With the K-Nearest neighbors algorithm now complete, it's time to apply it to a dataset! Let's work with the Iris dataset from the RDatasets package. First thing we need to do is read in the data, then we can subdivide it into *training data* and *test data.* The training data is what we will use within our K-Nearest Neighbors algorithm to predict the label of the test points, which will come from the data set aside for testing. For this exercise we will try to predict the species of iris using **SepalLength, SepalWidth,** and **PetalLength.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120-element CategoricalArray{String,1,UInt8}:\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " ⋮\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data and split 20% of it off for testing (10 obs/label)\n",
    "using RDatasets\n",
    "using Plots\n",
    "using CSV\n",
    "\n",
    "iris = dataset(\"datasets\",\"iris\")\n",
    "\n",
    "# Create array of data for variables of interest\n",
    "training_data = [x for x in zip(iris.SepalLength[1:40], iris.SepalWidth[1:40], iris.PetalLength[1:40])]\n",
    "append!(training_data, (iris.SepalLength[i], iris.SepalWidth[i], iris.PetalLength[i]) for i = 51:90)\n",
    "append!(training_data, (iris.SepalLength[i], iris.SepalWidth[i], iris.PetalLength[i]) for i = 101:140)\n",
    "\n",
    "# Create array of test data\n",
    "test_data = [x for x in zip(iris.SepalLength[41:50], iris.SepalWidth[41:50], iris.PetalLength[41:50])]\n",
    "append!(test_data, (iris.SepalLength[i], iris.SepalWidth[i], iris.PetalLength[i]) for i = 91:100)\n",
    "append!(test_data, (iris.SepalLength[i], iris.SepalWidth[i], iris.PetalLength[i]) for i = 141:150)\n",
    "\n",
    "# Create array of labels\n",
    "label_data = [iris.Species[i] for i = 1:40]\n",
    "append!(label_data, iris.Species[i] for i = 51:90)\n",
    "append!(label_data, iris.Species[i] for i = 101:140)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Since we know the labels of every point in our test set, it's very easy to test the accuracy of our K-Nearest Neighbors algorithm. With the knowledge that entries 21 through 30 are all *virginica*, let's use entry 26 to test our algorithm and see what label it predicts for the test point.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Algorithm Results:\n",
      "((6.7, 3.0, 5.0), CategoricalValue{String,UInt8} \"versicolor\", 0.20000000000000018)\n",
      "((6.9, 3.1, 5.4), CategoricalValue{String,UInt8} \"virginica\", 0.30000000000000027)\n",
      "((6.5, 3.2, 5.1), CategoricalValue{String,UInt8} \"virginica\", 0.30000000000000043)\n",
      "((6.8, 3.0, 5.5), CategoricalValue{String,UInt8} \"virginica\", 0.31622776601683766)\n",
      "\n",
      "Actual Species of Test Point:\n",
      "virginica\n"
     ]
    }
   ],
   "source": [
    "# Testing a point\n",
    "println(\"KNN Algorithm Results:\")\n",
    "KNN_result = KNN(test_data[26], training_data, label_data, 4)\n",
    "for i in 1:length(KNN_result)\n",
    "    println(result[i])\n",
    "end\n",
    "\n",
    "println(\"\\nActual Species of Test Point:\")\n",
    "println(iris.Species[146])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Of the 4 closest data points, one of them is a *versicolor* iris, while three are *virginica* irises, therefore the test point is most likely a *virginica* iris. Once we check the actual label of our test point, we see that it is in fact a *virginica* iris and that our algorithm predicted it correctly. We can further automate this algorithm by creating a function that counts the labels of the neighbors and simply returns the label of the majority.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNN_prediction (generic function with 1 method)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to interpret KNN algorithm results\n",
    "function KNN_prediction(KNN_result)\n",
    "    setosa_count = 0\n",
    "    versicolor_count = 0\n",
    "    virginica_count = 0\n",
    "    \n",
    "    for i in 1:length(KNN_result)\n",
    "        if (KNN_result[i][2] == \"setosa\")\n",
    "            setosa_count += 1\n",
    "        elseif (KNN_result[i][2] == \"versicolor\")\n",
    "            versicolor_count += 1\n",
    "        elseif (KNN_result[i][2] == \"virginica\")\n",
    "            virginica_count += 1\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Determine greatest count\n",
    "    if (virginica_count>versicolor_count && virginica_count>setosa_count)\n",
    "        return \"virginica\"\n",
    "    elseif (setosa_count>virginica_count && setosa_count>versicolor_count)\n",
    "        return \"setosa\"\n",
    "    elseif (versicolor_count>virginica_count && versicolor_count>setosa_count)\n",
    "        return \"versicolor\"\n",
    "    else\n",
    "        return \"inconclusive\" # used when there is a tie for greatest count\n",
    "    end\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors:\n",
      "((6.8, 3.0, 5.5), CategoricalValue{String,UInt8} \"virginica\", 0.17320508075688737)\n",
      "((6.7, 3.3, 5.7), CategoricalValue{String,UInt8} \"virginica\", 0.22360679774997896)\n",
      "((6.5, 3.0, 5.5), CategoricalValue{String,UInt8} \"virginica\", 0.24494897427831783)\n",
      "((6.9, 3.2, 5.7), CategoricalValue{String,UInt8} \"virginica\", 0.24494897427831822)\n",
      "\n",
      "Prediction:\n",
      "virginica\n"
     ]
    }
   ],
   "source": [
    "println(\"K-Nearest Neighbors:\")\n",
    "KNN_result = KNN(test_data[21], training_data, label_data, 4)\n",
    "for i in 1:length(KNN_result)\n",
    "    println(KNN_result[i])\n",
    "end\n",
    "\n",
    "println(\"\\nPrediction:\")\n",
    "println(KNN_prediction(KNN_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "While this automation can make predictions on a larger scale easier as it removes the need more manual interpretation of results, there are certainly applications of the K-Nearest Neighbors algorithm that this type of automation is not appropriate for. One example of such an application where you would want to see all of the results with no automated interpretations would be an algorithm that recommends movies based on movies that you like. Here's an example of this type of application using some sample data from a listing of movies.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>MovieID</th><th>MovieName</th><th>IMDBRating</th><th>Biography</th><th>Drama</th><th>Thriller</th></tr><tr><th></th><th>Int64</th><th>String</th><th>Float64</th><th>Int64</th><th>Int64</th><th>Int64</th></tr></thead><tbody><p>30 rows × 11 columns (omitted printing of 5 columns)</p><tr><th>1</th><td>58</td><td>The Imitation Game</td><td>8.0</td><td>1</td><td>1</td><td>1</td></tr><tr><th>2</th><td>8</td><td>Ex Machina</td><td>7.7</td><td>0</td><td>1</td><td>0</td></tr><tr><th>3</th><td>46</td><td>A Beautiful Mind</td><td>8.2</td><td>1</td><td>1</td><td>0</td></tr><tr><th>4</th><td>62</td><td>Good Will Hunting</td><td>8.3</td><td>0</td><td>1</td><td>0</td></tr><tr><th>5</th><td>97</td><td>Forrest Gump</td><td>8.8</td><td>0</td><td>1</td><td>0</td></tr><tr><th>6</th><td>98</td><td>21</td><td>6.8</td><td>0</td><td>1</td><td>0</td></tr><tr><th>7</th><td>31</td><td>Gifted</td><td>7.6</td><td>0</td><td>1</td><td>0</td></tr><tr><th>8</th><td>3</td><td>Travelling Salesman</td><td>5.9</td><td>0</td><td>1</td><td>0</td></tr><tr><th>9</th><td>51</td><td>Avatar</td><td>7.9</td><td>0</td><td>0</td><td>0</td></tr><tr><th>10</th><td>47</td><td>The Karate Kid</td><td>7.2</td><td>0</td><td>1</td><td>0</td></tr><tr><th>11</th><td>50</td><td>A Brilliant Young Mind</td><td>7.2</td><td>0</td><td>1</td><td>0</td></tr><tr><th>12</th><td>49</td><td>A Time To Kill</td><td>7.4</td><td>0</td><td>1</td><td>1</td></tr><tr><th>13</th><td>30</td><td>Interstellar</td><td>8.6</td><td>0</td><td>1</td><td>0</td></tr><tr><th>14</th><td>94</td><td>The Wolf of Wall Street</td><td>8.2</td><td>1</td><td>0</td><td>0</td></tr><tr><th>15</th><td>6</td><td>Black Panther</td><td>7.4</td><td>0</td><td>0</td><td>0</td></tr><tr><th>16</th><td>73</td><td>Inception</td><td>8.8</td><td>0</td><td>0</td><td>0</td></tr><tr><th>17</th><td>44</td><td>The Wind Rises</td><td>7.8</td><td>1</td><td>1</td><td>0</td></tr><tr><th>18</th><td>65</td><td>Spirited Away</td><td>8.6</td><td>0</td><td>0</td><td>0</td></tr><tr><th>19</th><td>48</td><td>Finding Forrester</td><td>7.3</td><td>0</td><td>1</td><td>0</td></tr><tr><th>20</th><td>27</td><td>The Fountain</td><td>7.3</td><td>0</td><td>0</td><td>0</td></tr><tr><th>21</th><td>57</td><td>The DaVinci Code</td><td>6.6</td><td>0</td><td>0</td><td>1</td></tr><tr><th>22</th><td>57</td><td>Stand and Deliver</td><td>7.3</td><td>0</td><td>1</td><td>0</td></tr><tr><th>23</th><td>14</td><td>The Terminator</td><td>8.0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>24</th><td>69</td><td>21 Jump Street</td><td>7.2</td><td>0</td><td>0</td><td>0</td></tr><tr><th>25</th><td>98</td><td>The Avengers</td><td>8.1</td><td>0</td><td>0</td><td>0</td></tr><tr><th>26</th><td>17</td><td>Thor: Ragnarok</td><td>7.9</td><td>0</td><td>0</td><td>0</td></tr><tr><th>27</th><td>12</td><td>Spirit: Stallion of the Cimarron</td><td>7.1</td><td>0</td><td>0</td><td>0</td></tr><tr><th>28</th><td>1</td><td>Hacksaw Ridge</td><td>8.2</td><td>1</td><td>1</td><td>0</td></tr><tr><th>29</th><td>86</td><td>12 Years a Slave</td><td>8.1</td><td>1</td><td>1</td><td>0</td></tr><tr><th>30</th><td>46</td><td>Queen of Katwe</td><td>7.4</td><td>1</td><td>1</td><td>0</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& MovieID & MovieName & IMDBRating & Biography & Drama & Thriller & \\\\\n",
       "\t\\hline\n",
       "\t& Int64 & String & Float64 & Int64 & Int64 & Int64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 58 & The Imitation Game & 8.0 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t2 & 8 & Ex Machina & 7.7 & 0 & 1 & 0 & $\\dots$ \\\\\n",
       "\t3 & 46 & A Beautiful Mind & 8.2 & 1 & 1 & 0 & $\\dots$ \\\\\n",
       "\t4 & 62 & Good Will Hunting & 8.3 & 0 & 1 & 0 & $\\dots$ \\\\\n",
       "\t5 & 97 & Forrest Gump & 8.8 & 0 & 1 & 0 & $\\dots$ \\\\\n",
       "\t6 & 98 & 21 & 6.8 & 0 & 1 & 0 & $\\dots$ \\\\\n",
       "\t7 & 31 & Gifted & 7.6 & 0 & 1 & 0 & $\\dots$ \\\\\n",
       "\t8 & 3 & Travelling Salesman & 5.9 & 0 & 1 & 0 & $\\dots$ \\\\\n",
       "\t9 & 51 & Avatar & 7.9 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t10 & 47 & The Karate Kid & 7.2 & 0 & 1 & 0 & $\\dots$ \\\\\n",
       "\t11 & 50 & A Brilliant Young Mind & 7.2 & 0 & 1 & 0 & $\\dots$ \\\\\n",
       "\t12 & 49 & A Time To Kill & 7.4 & 0 & 1 & 1 & $\\dots$ \\\\\n",
       "\t13 & 30 & Interstellar & 8.6 & 0 & 1 & 0 & $\\dots$ \\\\\n",
       "\t14 & 94 & The Wolf of Wall Street & 8.2 & 1 & 0 & 0 & $\\dots$ \\\\\n",
       "\t15 & 6 & Black Panther & 7.4 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t16 & 73 & Inception & 8.8 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t17 & 44 & The Wind Rises & 7.8 & 1 & 1 & 0 & $\\dots$ \\\\\n",
       "\t18 & 65 & Spirited Away & 8.6 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t19 & 48 & Finding Forrester & 7.3 & 0 & 1 & 0 & $\\dots$ \\\\\n",
       "\t20 & 27 & The Fountain & 7.3 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t21 & 57 & The DaVinci Code & 6.6 & 0 & 0 & 1 & $\\dots$ \\\\\n",
       "\t22 & 57 & Stand and Deliver & 7.3 & 0 & 1 & 0 & $\\dots$ \\\\\n",
       "\t23 & 14 & The Terminator & 8.0 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t24 & 69 & 21 Jump Street & 7.2 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t25 & 98 & The Avengers & 8.1 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t26 & 17 & Thor: Ragnarok & 7.9 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t27 & 12 & Spirit: Stallion of the Cimarron & 7.1 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t28 & 1 & Hacksaw Ridge & 8.2 & 1 & 1 & 0 & $\\dots$ \\\\\n",
       "\t29 & 86 & 12 Years a Slave & 8.1 & 1 & 1 & 0 & $\\dots$ \\\\\n",
       "\t30 & 46 & Queen of Katwe & 7.4 & 1 & 1 & 0 & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "30×11 DataFrame. Omitted printing of 7 columns\n",
       "│ Row │ MovieID │ MovieName                        │ IMDBRating │ Biography │\n",
       "│     │ \u001b[90mInt64\u001b[39m   │ \u001b[90mString\u001b[39m                           │ \u001b[90mFloat64\u001b[39m    │ \u001b[90mInt64\u001b[39m     │\n",
       "├─────┼─────────┼──────────────────────────────────┼────────────┼───────────┤\n",
       "│ 1   │ 58      │ The Imitation Game               │ 8.0        │ 1         │\n",
       "│ 2   │ 8       │ Ex Machina                       │ 7.7        │ 0         │\n",
       "│ 3   │ 46      │ A Beautiful Mind                 │ 8.2        │ 1         │\n",
       "│ 4   │ 62      │ Good Will Hunting                │ 8.3        │ 0         │\n",
       "│ 5   │ 97      │ Forrest Gump                     │ 8.8        │ 0         │\n",
       "│ 6   │ 98      │ 21                               │ 6.8        │ 0         │\n",
       "│ 7   │ 31      │ Gifted                           │ 7.6        │ 0         │\n",
       "│ 8   │ 3       │ Travelling Salesman              │ 5.9        │ 0         │\n",
       "│ 9   │ 51      │ Avatar                           │ 7.9        │ 0         │\n",
       "│ 10  │ 47      │ The Karate Kid                   │ 7.2        │ 0         │\n",
       "⋮\n",
       "│ 20  │ 27      │ The Fountain                     │ 7.3        │ 0         │\n",
       "│ 21  │ 57      │ The DaVinci Code                 │ 6.6        │ 0         │\n",
       "│ 22  │ 57      │ Stand and Deliver                │ 7.3        │ 0         │\n",
       "│ 23  │ 14      │ The Terminator                   │ 8.0        │ 0         │\n",
       "│ 24  │ 69      │ 21 Jump Street                   │ 7.2        │ 0         │\n",
       "│ 25  │ 98      │ The Avengers                     │ 8.1        │ 0         │\n",
       "│ 26  │ 17      │ Thor: Ragnarok                   │ 7.9        │ 0         │\n",
       "│ 27  │ 12      │ Spirit: Stallion of the Cimarron │ 7.1        │ 0         │\n",
       "│ 28  │ 1       │ Hacksaw Ridge                    │ 8.2        │ 1         │\n",
       "│ 29  │ 86      │ 12 Years a Slave                 │ 8.1        │ 1         │\n",
       "│ 30  │ 46      │ Queen of Katwe                   │ 7.4        │ 1         │"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in movie data\n",
    "movie_data = CSV.read(\"movies_recommendation_data.csv\")\n",
    "# Run KNN algorithm and output results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "For this data, the variables we will be using for the x_data are the genre columns along with the IMDB rating, and the label data will be the movie names. When this data is run through the K-Nearest Neighbors algorithm, it will return the *k* nearest movies to it. In other words, the algorithm will take the movie that you feed in and return the movies that are most similar and you are most likely to enjoy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
