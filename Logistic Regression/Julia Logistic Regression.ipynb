{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic regression** is a predictive machine learning algorithm that attempts to predict an outcome. A logistic regression algorithm consists of three parts: **Estimation, Loss Function Evaluation,** and **Optimization** (also referred to as **Training**).\n",
    "\n",
    "### Optimization\n",
    "Similarly to **linear regression**, logistic regression uses a vector consisting of weights and a bias in order to evaluate $z$, which is found through the equation: \n",
    "$$z=w^Tx+b$$ \n",
    "\n",
    "The values of $z$ are then fed into $\\sigma$, otherwise known as the **sigmoid function**, in order to obtain a prediction for the value of $y$, also referred to as $\\hat y$. The sigmoid function is shown by the following equation:\n",
    "$$\\sigma(z^i)=\\frac{1}{1+e^{-(w^Tx+b)}}$$\n",
    "\n",
    "For certain applications, such as the one we will evaluating in this demonstration, $y$ is a binary value with $y=1$ indicating success and $y=0$ indicating failure. In these situations, $\\hat y$ will be rounded either up or down depending on its value in order to provide a prediction of the success or failure for the provided data.\n",
    "\n",
    "\n",
    "### Loss Function Evaluation\n",
    "Once we have estimated values, we then need to evaluate and optimize the accuracy of our model. The first step to performing this optimization is to evaluate model efficacy through use of a **loss function** which will tell us how close our estimation, $\\hat y$, is to the actual value for the data, $y$. \n",
    "\n",
    "**Loss function:** $L(\\hat y^i, y^i)$ = How close $\\hat y^i$ is to y \n",
    "$$P(y|x)=\\hat y^y*(1-\\hat y)^{1-y}$$\n",
    "$$log(P(y|x))=y(log(\\hat y))+(1-y)(log(1 - \\hat y))$$\n",
    "**Cross Entropy Loss Function:**\n",
    "$$L_{CE}(\\hat y, y)=-[y(log(\\hat y))+(1-y)(log(1 - \\hat y))]$$\n",
    "Since $\\hat y$=$\\sigma (z)$:\n",
    "$$L_{CE}(\\sigma (z), y)=-[y(log(\\sigma (z)))+(1-y)(log(1 - \\sigma (z)))]$$\n",
    "$$L_{CE}(w, b)=-[y(log(\\sigma (w^Tx+b)))+(1-y)(log(1 - \\sigma (w^Tx+b)))]$$\n",
    "\n",
    "\n",
    "### Optimization or Training\n",
    "For our optimization method, we will be using **gradient descent** to minimize the value of the cross entropy loss function, which means we will be training our model using the entire dataset. The first thing we need to do is take the partial derivatives of the function to be optimized which gives us the following:\n",
    "\n",
    "$$\\frac{\\delta L_{CE}}{\\delta w_j} = [\\sigma (wx+b)-y]x_j$$\n",
    "$$\\frac{\\delta L_{CE}}{\\delta b} = \\sigma (wx+b)-y$$\n",
    "\n",
    "These partial derivatives will then allow us to find create our optimization equations which gives us the following equations for a given point $(x^i,y^i)$:\n",
    "$$w^{k+1}_j=w^k_j+\\alpha \\frac{\\delta L_{CE}}{\\delta w_j^k}(w^k b^k) = w^k_j+\\alpha ([\\sigma (w^k_jx^i+b)-y^i]x^i)$$\n",
    "\n",
    "\n",
    "$$b^{k+1} = b^k+\\alpha \\frac{\\delta L_{CE}}{\\delta b}(w^k b^k) = b^k+\\alpha [\\sigma (wx^i+b)-y^i]$$\n",
    "\n",
    "By iterating the above equations we will be able to minimize the values of the cross entropy loss function and train our model to more accurately predict the result based on the given data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "For this demonstration of logistic regression we will be using sample student data in order to predict whether a student will be admitted into a university. First thing's first, let's load up and look at our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>gmat</th><th>gpa</th><th>work_experience</th><th>admitted</th></tr><tr><th></th><th>Int64</th><th>Float64</th><th>Int64</th><th>Int64</th></tr></thead><tbody><p>5 rows × 4 columns</p><tr><th>1</th><td>780</td><td>4.0</td><td>3</td><td>1</td></tr><tr><th>2</th><td>750</td><td>3.9</td><td>4</td><td>1</td></tr><tr><th>3</th><td>690</td><td>3.3</td><td>3</td><td>0</td></tr><tr><th>4</th><td>710</td><td>3.7</td><td>5</td><td>1</td></tr><tr><th>5</th><td>680</td><td>3.9</td><td>4</td><td>0</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccc}\n",
       "\t& gmat & gpa & work\\_experience & admitted\\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Float64 & Int64 & Int64\\\\\n",
       "\t\\hline\n",
       "\t1 & 780 & 4.0 & 3 & 1 \\\\\n",
       "\t2 & 750 & 3.9 & 4 & 1 \\\\\n",
       "\t3 & 690 & 3.3 & 3 & 0 \\\\\n",
       "\t4 & 710 & 3.7 & 5 & 1 \\\\\n",
       "\t5 & 680 & 3.9 & 4 & 0 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "5×4 DataFrame\n",
       "│ Row │ gmat  │ gpa     │ work_experience │ admitted │\n",
       "│     │ \u001b[90mInt64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mInt64\u001b[39m           │ \u001b[90mInt64\u001b[39m    │\n",
       "├─────┼───────┼─────────┼─────────────────┼──────────┤\n",
       "│ 1   │ 780   │ 4.0     │ 3               │ 1        │\n",
       "│ 2   │ 750   │ 3.9     │ 4               │ 1        │\n",
       "│ 3   │ 690   │ 3.3     │ 3               │ 0        │\n",
       "│ 4   │ 710   │ 3.7     │ 5               │ 1        │\n",
       "│ 5   │ 680   │ 3.9     │ 4               │ 0        │"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load necessary packages\n",
    "using CSV\n",
    "\n",
    "# Read in & look at data structure\n",
    "data = CSV.read(\"candidates_data (1).csv\")\n",
    "data[1:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of 4 variables; *gmat*, *gpa*, *work_experience*, and *admitted*. We need to separate this data into **feature data** which will be used to train our model, and **label data** which is the outcome that we are trying to predict. Since we are trying to predict whether the student will be admitted into the university, our label data is clearly the *admitted* data. This leaves *gmat*, *gpa*, and *work_experience* as our feature data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data:\n",
    "# Create array of feature data\n",
    "feature_data = [[x[1], x[2], x[3]] for x in zip(data.gmat, data.gpa, data.work_experience)]\n",
    "\n",
    "# Create array of label data\n",
    "label_data = [x for x in data.admitted];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "average_cost (generic function with 1 method)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "σ(x) = 1/(1+exp(-x))\n",
    "\n",
    "# Cross entropy loss function to determine accuracy of predictions\n",
    "function cross_entropy_loss(x, y, w, b)\n",
    "    return -y*log(σ(w'x +b)) -(1-y)*log(1 - σ(w'x+b))\n",
    "end\n",
    "\n",
    "# Cost function to measure error of weights & biases\n",
    "function average_cost(features, labels, w, b)\n",
    "    N = length(features)\n",
    "    return (1/N)*sum((cross_entropy_loss(features[i], labels[i], w, b) for i = 1:N))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "There are two types of gradient descent, **batch** and **stochastic**. **Batch gradient descent** uses the entire dataset to train the model, while **stochastic gradient descent** randomly selects points to use for training each iteration. While batch gradient descent will become optimized with fewer iterations, it is more computationally expensive than stochastic. However, our dataset is small enough that this is not a concern so we will be using the batch method.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "batch_gradient_descent (generic function with 1 method)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function batch_gradient_descent(features, labels, w, b, α)\n",
    "    \n",
    "    del_w = [0.0 for i = 1:length(w)]\n",
    "    del_b = 0.0\n",
    "    \n",
    "    N = length(features)\n",
    "    \n",
    "    for i = 1:N\n",
    "        del_w += (σ(w'features[i]+b) - labels[i])*features[i]\n",
    "        del_b += (σ(w'features[i]+b) - labels[i])\n",
    "    end\n",
    "    \n",
    "    w = w - α*del_w\n",
    "    b = b - α*del_b\n",
    "    \n",
    "    return w, b\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now that our gradient descent function is complete, we can set our starting weights and bias along with a step size to run a couple test iterations to be sure our function is decreasing the cost of our model as it iterates.\n",
    "        \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cost: 0.6931471805599446\n",
      "\n",
      "After 1 iteration at α=0.00001: 0.7652556019981452\n",
      "\n",
      "After 1 iteration at α=0.0000001: 0.6931177157407157\n"
     ]
    }
   ],
   "source": [
    "# Set starting weights and biases\n",
    "w = [0.0, 0.0, 0.0]\n",
    "b = 0.0\n",
    "println(\"Starting cost: \", average_cost(feature_data, label_data, w, b))\n",
    "\n",
    "# This step size is too big!\n",
    "w, b = batch_gradient_descent(feature_data, label_data, w, b, 0.00001)\n",
    "println(\"\\nAfter 1 iteration at α=0.00001: \", average_cost(feature_data, label_data, w, b))\n",
    "\n",
    "w = [0.0, 0.0, 0.0]\n",
    "b = 0.0\n",
    "# This is better, but let's try a little bigger\n",
    "w, b = batch_gradient_descent(feature_data, label_data, w, b, 0.0000001)\n",
    "println(\"\\nAfter 1 iteration at α=0.0000001: \", average_cost(feature_data, label_data, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now that we've testing some step sizes and found what we want to go with, we can now create a function to iterate our gradient descent function as many times as we want. One measure we can implement to verify our descent algorithm is decreasing cost throught all the iterations is to include checkpoints to print out the cost after a set number of iterations. Once we have iterated the descent function the desired number of times we will output the final weights and biases for our regression function.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at iteration 1: 0.6931066511441942\n",
      "Cost at iteration 100: 0.6926384571579437\n",
      "Cost at iteration 10000: 0.6510831083097195\n",
      "Cost at iteration 100000: 0.4899247448010744\n",
      "Cost at iteration 1000000: 0.3980123186961089\n",
      "\n",
      "Final Weights: [-0.009233111010187943, 0.8564526529532422, 1.0719001573543308]\n",
      "Final Bias: -0.35748882940581317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.009233111010187943, 0.8564526529532422, 1.0719001573543308], -0.35748882940581317)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function batch_train(features, labels, w, b, α, iter)\n",
    "    for i = 1:iter\n",
    "        w,b = batch_gradient_descent(feature_data, label_data, w, b, α)\n",
    "        if i==1\n",
    "            println(\"Cost at iteration \",i,\": \", average_cost(feature_data, label_data, w, b))\n",
    "        end\n",
    "        if i==100\n",
    "            println(\"Cost at iteration \",i,\": \", average_cost(feature_data, label_data, w, b))\n",
    "        end\n",
    "        if i==10000\n",
    "            println(\"Cost at iteration \",i,\": \", average_cost(feature_data, label_data, w, b))\n",
    "        end\n",
    "        if i==100000\n",
    "            println(\"Cost at iteration \",i,\": \", average_cost(feature_data, label_data, w, b))\n",
    "        end\n",
    "        if i==1000000\n",
    "            println(\"Cost at iteration \",i,\": \", average_cost(feature_data, label_data, w, b))\n",
    "        end\n",
    "    end\n",
    "    println(\"\\nFinal Weights: \", w)\n",
    "    println(\"Final Bias: \", b)\n",
    "    return w, b\n",
    "end\n",
    "\n",
    "# Train our model using 1,000,000 iterations \n",
    "w, b = batch_train(feature_data, label_data, w, b, 0.0000004, 1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now that our model is trained, we can create a function to test it by having it predict whether a student will be accepted into the university based on their *GPA*, *GMAT score*, and *work experience*. Once we have our predictions, we can compare them to the actual labels and determine the accuracy of our trained model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 82.5%\n"
     ]
    }
   ],
   "source": [
    "# Predict acceptance\n",
    "function predict(x, w, b)\n",
    "   if σ(w'x+b) >= 0.5\n",
    "        return 1\n",
    "    else\n",
    "        return 0\n",
    "    end\n",
    "end\n",
    "    \n",
    "# Compare prediction to actual acceptance data to find error rate\n",
    "function model_accuracy(x, y, w, b)\n",
    "    mean_error = 0\n",
    "    for i in 1:length(x)\n",
    "        mean_error += (predict(x[i], w, b) - y[i])^2\n",
    "    end\n",
    "    println(\"Model Accuracy: \", (1-(mean_error/length(x)))*100,\"%\")\n",
    "end\n",
    "\n",
    "model_accuracy(feature_data, label_data, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "So after training our regression model via 1,000,000 iterations of gradient descent optimization we were able to achieve an 82.5% accuracy when predicting the acceptance of students at a particular university when given their *GPA*, *GMAT scores*, and *work experience.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
